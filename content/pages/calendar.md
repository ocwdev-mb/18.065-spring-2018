---
content_type: page
description: This section includes lecture schedule and titles.
learning_resource_types: []
ocw_type: CourseSection
title: Calendar
uid: b14c2a39-0eb1-ee2d-e5c9-8efe1bcd61f7
---

{{< tableopen >}}
{{< theadopen >}}
{{< tropen >}}
{{< thopen >}}
LEC #
{{< thclose >}}
{{< thopen >}}
TOPICS
{{< thclose >}}

{{< trclose >}}

{{< theadclose >}}
{{< tropen >}}
{{< tdopen >}}
1
{{< tdclose >}}
{{< tdopen >}}
The Column Space of \\(A\\) Contains All Vectors \\(A\\boldsymbol{x}\\)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
2
{{< tdclose >}}
{{< tdopen >}}
Multiplying and Factoring Matrices 
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
3
{{< tdclose >}}
{{< tdopen >}}
Orthonormal Columns in \\(Q\\) Give \\(Q’Q= I\\)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
4
{{< tdclose >}}
{{< tdopen >}}
Eigenvalues and Eigenvectors
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
5
{{< tdclose >}}
{{< tdopen >}}
Positive Definite and Semidefinite Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
6
{{< tdclose >}}
{{< tdopen >}}
Singular Value Decomposition (SVD)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
7
{{< tdclose >}}
{{< tdopen >}}
Eckart-Young: The Closest Rank \\(k\\) Matrix to \\(A\\)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
8
{{< tdclose >}}
{{< tdopen >}}
Norms of Vectors and Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
9
{{< tdclose >}}
{{< tdopen >}}
Four Ways to Solve Least Squares Problems
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
10
{{< tdclose >}}
{{< tdopen >}}
Survey of Difficulties with \\(A\\boldsymbol{x} = \\boldsymbol{b}\\)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
11
{{< tdclose >}}
{{< tdopen >}}
Minimizing \\(‖\\boldsymbol{x}‖\\) Subject to \\(A\\boldsymbol{x} = \\boldsymbol{b}\\)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
12
{{< tdclose >}}
{{< tdopen >}}
Computing Eigenvalues and Singular Values
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
13
{{< tdclose >}}
{{< tdopen >}}
Randomized Matrix Multiplication
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
14
{{< tdclose >}}
{{< tdopen >}}
Low Rank Changes in \\(A\\) and Its Inverse
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
15
{{< tdclose >}}
{{< tdopen >}}
Matrices \\(A(t)\\) Depending on \\(t\\), Derivative = \\(dA/dt\\)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
16
{{< tdclose >}}
{{< tdopen >}}
Derivatives of Inverse and Singular Values
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
17
{{< tdclose >}}
{{< tdopen >}}
Rapidly Decreasing Singular Values
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
18
{{< tdclose >}}
{{< tdopen >}}
Counting Parameters in SVD, LU, QR, Saddle Points
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
19
{{< tdclose >}}
{{< tdopen >}}
Saddle Points Continued, Maxmin Principle
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
20
{{< tdclose >}}
{{< tdopen >}}
Definitions and Inequalities
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
21
{{< tdclose >}}
{{< tdopen >}}
Minimizing a Function Step by Step
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
22
{{< tdclose >}}
{{< tdopen >}}
Gradient Descent: Downhill to a Minimum
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
23
{{< tdclose >}}
{{< tdopen >}}
Accelerating Gradient Descent (Use Momentum)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
24
{{< tdclose >}}
{{< tdopen >}}
Linear Programming and Two-Person Games
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
25
{{< tdclose >}}
{{< tdopen >}}
Stochastic Gradient Descent
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
26
{{< tdclose >}}
{{< tdopen >}}
Structure of Neural Nets for Deep Learning
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
27
{{< tdclose >}}
{{< tdopen >}}
Backpropagation: Find Partial Derivatives
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
28
{{< tdclose >}}
{{< tdopen >}}
Computing in Class \[No video available\]
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
29
{{< tdclose >}}
{{< tdopen >}}
Computing in Class (cont.) \[No video available\]
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
30
{{< tdclose >}}
{{< tdopen >}}
Completing a Rank-One Matrix, Circulants!
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
31
{{< tdclose >}}
{{< tdopen >}}
Eigenvectors of Circulant Matrices: Fourier Matrix
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
32
{{< tdclose >}}
{{< tdopen >}}
ImageNet is a Convolutional Neural Network (CNN), The Convolution Rule
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
33
{{< tdclose >}}
{{< tdopen >}}
Neural Nets and the Learning Function
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
34
{{< tdclose >}}
{{< tdopen >}}
Distance Matrices, Procrustes Problem
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
35
{{< tdclose >}}
{{< tdopen >}}
Finding Clusters in Graphs
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
36
{{< tdclose >}}
{{< tdopen >}}
Alan Edelman and Julia Language
{{< tdclose >}}

{{< trclose >}}

{{< tableclose >}}