---
content_type: page
description: This section includes a detailed reading list.
learning_resource_types:
- Readings
ocw_type: CourseSection
title: Readings
uid: daef5f63-da5b-36f5-ee6b-f28e71eb7656
---

Reading assignments are all in the textbook: Strang, Gilbert. _[Linear Algebra and Learning from Data](http://math.mit.edu/~gs/learningfromdata/)_. Wellesley-Cambridge Press, 2018. ISBN: 9780692196380.

Professor Strang created [a website for the book](http://math.mit.edu/~gs/learningfromdata/), including a link to the [Table of Contents (PDF)](http://math.mit.edu/%7Egs/learningfromdata/dsla_toc.pdf) and sample chapters.

{{< tableopen >}}
{{< theadopen >}}
{{< tropen >}}
{{< thopen >}}
LEC #
{{< thclose >}}
{{< thopen >}}
TOPICS
{{< thclose >}}
{{< thopen >}}
READINGS
{{< thclose >}}

{{< trclose >}}

{{< theadclose >}}
{{< tropen >}}
{{< tdopen >}}
1
{{< tdclose >}}
{{< tdopen >}}
The Column Space of \\(A\\) Contains All Vectors \\(A\\boldsymbol{x}\\)
{{< tdclose >}}
{{< tdopen >}}
Section I.1: Multiplication \\(A\\boldsymbol{x}\\) Using Columns of \\(A\\)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
2
{{< tdclose >}}
{{< tdopen >}}
Multiplying and Factoring Matrices 
{{< tdclose >}}
{{< tdopen >}}
Section I.2: Matrix-Matrix Multiplication \\(AB\\)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
3
{{< tdclose >}}
{{< tdopen >}}
Orthonormal Columns in \\(Q\\) Give \\(Q’Q= I\\)
{{< tdclose >}}
{{< tdopen >}}
Section I.5: Orthogonal Matrices and Subspaces
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
4
{{< tdclose >}}
{{< tdopen >}}
Eigenvalues and Eigenvectors
{{< tdclose >}}
{{< tdopen >}}
Section I.6: Eigenvalues and Eigenvectors
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
5
{{< tdclose >}}
{{< tdopen >}}
Positive Definite and Semidefinite Matrices
{{< tdclose >}}
{{< tdopen >}}
Section I.7: Symmetric Positive Definite Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
6
{{< tdclose >}}
{{< tdopen >}}
Singular Value Decomposition (SVD)
{{< tdclose >}}
{{< tdopen >}}
Section I.8: Singular Values and Singular Vectors in the SVD
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
7
{{< tdclose >}}
{{< tdopen >}}
Eckart-Young: The Closest Rank \\(k\\) Matrix to \\(A\\)
{{< tdclose >}}
{{< tdopen >}}
Section I.9: Principal Components and the Best Low Rank Matrix
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
8
{{< tdclose >}}
{{< tdopen >}}
Norms of Vectors and Matrices
{{< tdclose >}}
{{< tdopen >}}
Section I.11: Norms of Vectors and Functions and Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
9
{{< tdclose >}}
{{< tdopen >}}
Four Ways to Solve Least Squares Problems
{{< tdclose >}}
{{< tdopen >}}
Section II.2: Least Squares: Four Ways
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
10
{{< tdclose >}}
{{< tdopen >}}
Survey of Difficulties with \\(A\\boldsymbol{x} = \\boldsymbol{b}\\)
{{< tdclose >}}
{{< tdopen >}}
Intro Chapter 2: Introduction to Computations with Large Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
11
{{< tdclose >}}
{{< tdopen >}}
Minimizing \\(‖\\boldsymbol{x}‖\\) Subject to \\(A\\boldsymbol{x} = \\boldsymbol{b}\\)
{{< tdclose >}}
{{< tdopen >}}
Section I.11: Norms of Vectors and Functions and Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
12
{{< tdclose >}}
{{< tdopen >}}
Computing Eigenvalues and Singular Values
{{< tdclose >}}
{{< tdopen >}}
Section II.1: Numerical Linear Algebra
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
13
{{< tdclose >}}
{{< tdopen >}}
Randomized Matrix Multiplication
{{< tdclose >}}
{{< tdopen >}}
Section II.4: Randomized Linear Algebra
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
14
{{< tdclose >}}
{{< tdopen >}}
Low Rank Changes in \\(A\\) and Its Inverse
{{< tdclose >}}
{{< tdopen >}}
Section III.1: Changes in \\(A^{-1}\\) from Changes in \\(A\\)
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
15
{{< tdclose >}}
{{< tdopen >}}
Matrices \\(A(t)\\) Depending on \\(t\\), Derivative = \\(dA/dt\\)
{{< tdclose >}}
{{< tdopen >}}
Section III.1: Changes in \\(A^{-1}\\) from Changes in \\(A\\)  
Section III.2: Interlacing Eigenvalues and Low Rank Signals
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
16
{{< tdclose >}}
{{< tdopen >}}
Derivatives of Inverse and Singular Values
{{< tdclose >}}
{{< tdopen >}}
Section III.1: Changes in \\(A^{-1}\\) from Changes in \\(A\\)  
Section III.2: Interlacing Eigenvalues and Low Rank Signals
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
17
{{< tdclose >}}
{{< tdopen >}}
Rapidly Decreasing Singular Values
{{< tdclose >}}
{{< tdopen >}}
Section III.3: Rapidly Decaying Singular Values
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
18
{{< tdclose >}}
{{< tdopen >}}
Counting Parameters in SVD, LU, QR, Saddle Points
{{< tdclose >}}
{{< tdopen >}}
Section III.2: Interlacing Eigenvalues and Low Rank Signals
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
19
{{< tdclose >}}
{{< tdopen >}}
Saddle Points Continued, Maxmin Principle
{{< tdclose >}}
{{< tdopen >}}
Section III.2: Interlacing Eigenvalues and Low Rank Signals  
Section V.1: Mean, Variance, and Probability
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
20
{{< tdclose >}}
{{< tdopen >}}
Definitions and Inequalities
{{< tdclose >}}
{{< tdopen >}}
 
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
21
{{< tdclose >}}
{{< tdopen >}}
Minimizing a Function Step by Step
{{< tdclose >}}
{{< tdopen >}}
Section VI.1: Minimum Problems: Convexity and Newton's Method  
Section VI.4: Gradient Descent Toward the Minimum
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
22
{{< tdclose >}}
{{< tdopen >}}
Gradient Descent: Downhill to a Minimum
{{< tdclose >}}
{{< tdopen >}}
Section VI.4: Gradient Descent Toward the Minimum
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
23
{{< tdclose >}}
{{< tdopen >}}
Accelerating Gradient Descent (Use Momentum)
{{< tdclose >}}
{{< tdopen >}}
Section VI.4: Gradient Descent Toward the Minimum
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
24
{{< tdclose >}}
{{< tdopen >}}
Linear Programming and Two-Person Games
{{< tdclose >}}
{{< tdopen >}}
Section VI.2: Lagrange Multipliers = Derivatives of the Cost  
Section VI.3: Linear Programming, Game Theory, and Duality
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
25
{{< tdclose >}}
{{< tdopen >}}
Stochastic Gradient Descent
{{< tdclose >}}
{{< tdopen >}}
Section VI.5: Stochastic Gradient Descent and ADAM
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
26
{{< tdclose >}}
{{< tdopen >}}
Structure of Neural Nets for Deep Learning
{{< tdclose >}}
{{< tdopen >}}
Section VII.1: The Construction of Deep Neural Networks
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
27
{{< tdclose >}}
{{< tdopen >}}
Backpropagation: Find Partial Derivatives
{{< tdclose >}}
{{< tdopen >}}
Section VII.3: Backpropagation and the Chain Rule
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
28
{{< tdclose >}}
{{< tdopen >}}
Computing in Class \[No video available\]
{{< tdclose >}}
{{< tdopen >}}
Section VII.2: Convolutional Neural Nets
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
29
{{< tdclose >}}
{{< tdopen >}}
Computing in Class (cont.) \[No video available\]
{{< tdclose >}}
{{< tdopen >}}
 
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
30
{{< tdclose >}}
{{< tdopen >}}
Completing a Rank One Matrix, Circulants!
{{< tdclose >}}
{{< tdopen >}}
Section IV.8: Completing Rank One Matrices  
Section IV.2: Shift Matrices and Circulant Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
31
{{< tdclose >}}
{{< tdopen >}}
Eigenvectors of Circulant Matrices: Fourier Matrix
{{< tdclose >}}
{{< tdopen >}}
Section IV.2: Shift Matrices and Circulant Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
32
{{< tdclose >}}
{{< tdopen >}}
ImageNet is a Convolutional Neural Network (CNN), The Convolution Rule
{{< tdclose >}}
{{< tdopen >}}
Section IV.2: Shift Matrices and Circulant Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
33
{{< tdclose >}}
{{< tdopen >}}
Neural Nets and the Learning Function
{{< tdclose >}}
{{< tdopen >}}
Section VII.1: The Construction of Deep Neural Networks  
Section IV.10: Distance Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
34
{{< tdclose >}}
{{< tdopen >}}
Distance Matrices, Procrustes Problem
{{< tdclose >}}
{{< tdopen >}}
Section IV.9: The Orthogonal Procrustes Problem  
Section IV.10: Distance Matrices
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
35
{{< tdclose >}}
{{< tdopen >}}
Finding Clusters in Graphs
{{< tdclose >}}
{{< tdopen >}}
Section IV.6: Graphs and Laplacians and Kirchhoff's Laws  
Section IV.7: Clustering by Spectral Methods and \\(k\\)-means
{{< tdclose >}}

{{< trclose >}}
{{< tropen >}}
{{< tdopen >}}
36
{{< tdclose >}}
{{< tdopen >}}
Alan Edelman and Julia Language
{{< tdclose >}}
{{< tdopen >}}
Section III.3: Rapidly Decaying Singular Values  
Section VII.2: Convolutional Neural Nets
{{< tdclose >}}

{{< trclose >}}

{{< tableclose >}}