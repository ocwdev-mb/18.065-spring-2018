---
content_type: resource
description: ''
draft: false
end_time: ''
file: null
file_type: ''
image_metadata:
  caption: ''
  credit: ''
  image-alt: ''
learning_resource_types:
- Lecture Videos
license: https://creativecommons.org/licenses/by-nc-sa/4.0/
ocw_type: ''
optional_tab_title: Problem Set
optional_text: "**Problems for Lecture 7  \nFrom textbook Section I.9**\n\n2\\. Find\
  \ a closest rank-1 approximation to these matrices (\\\\(L^2\\\\) or Frobenius norm)\
  \ :\n\n$$A = \\\\left\\[\\\\begin{matrix}3 & 0&0 \\\\\\\\ 0 &2&0\\\\\\\\ 0 & 0&1\\\
  \\end{matrix}\\\\right\\] \\\\hspace{12pt} A = \\\\left\\[\\\\begin{matrix}0 & 3\\\
  \\\\\\ 2 & 0\\\\end{matrix}\\\\right\\] \\\\hspace{12pt} A = \\\\left\\[\\\\begin{matrix}2\
  \ & 1\\\\\\\\ 1 & 2\\\\end{matrix}\\\\right\\] $$\n\n10\\. If \\\\(A\\\\) is a 2\
  \ by 2 matrix with \u03C31 \u2265 \u03C32 > 0, find \\\\(||A^{-1}||\\_2\\\\) and\
  \ \\\\(||A^{-1}||^2\\_F\\\\)."
parent_title: Video Lectures
parent_type: CourseSection
related_resources_text: ''
resource_index_text: ''
resourcetype: Video
start_time: ''
title: 'Lecture 7: Eckart-Young: The Closest Rank k Matrix to A'
uid: 7d689bdb-8da9-848c-569f-85e93ce9b9d4
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture07_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/ae3fcb7f64fd52e7abb899b9f665d5f4_Y4f7K9XF04k.vtt
  video_thumbnail_file: https://img.youtube.com/vi/Y4f7K9XF04k/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/70996912a170cf9c6ebb018f03c1fc85_Y4f7K9XF04k.pdf
video_metadata:
  youtube_id: Y4f7K9XF04k
---
Description

In this lecture, Professor Strang reviews Principal Component Analysis (PCA), which is a major tool in understanding a matrix of data. In particular, he focuses on the Eckart-Young low rank approximation theorem.

Summary

\\(A\_k = \\sigma\_1 u\_1 v^{\\mathtt{T}}\_1 + \\cdots + \\sigma\_k u\_k v^{\\mathtt{T}}\_k\\) (larger \\(\\sigma\\)'s from \\(A = U\\Sigma V^{\\mathtt{T}}\\))   
The norm of \\(A - A\_k\\) is below the norm of all other \\(A - B\_k\\).   
Frobenius norm squared = sum of squares of all entries   
The idea of Principal Component Analysis (PCA)

Related section in textbook: I.9

**Instructor:** Prof. Gilbert Strang