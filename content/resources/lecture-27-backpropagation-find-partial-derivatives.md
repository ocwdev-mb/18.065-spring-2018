---
content_type: resource
description: ''
end_time: ''
file: null
learning_resource_types:
- Lecture Videos
license: https://creativecommons.org/licenses/by-nc-sa/4.0/
ocw_type: ''
optional_tab_title: Problem Set
optional_text: "**Problems for Lecture 27  \nFrom textbook Section VII.2**\n\n2\\\
  . If \\\\(A\\\\) is an \\\\(m\\\\) by \\\\(n\\\\) matrix with \\\\(m>n\\\\), is\
  \ it faster to multiply \\\\(A(A^{\\\\mathtt{T}} A)\\\\) or \\\\((AA^{\\\\mathtt{T}})A\\\
  \\)?\n\n5\\. Draw a computational graph to compute the function \\\\(f(x,y)=x^3(x-y)\\\
  \\). Use the graph to compute \\\\(f(2,3)\\\\)."
parent_title: Video Lectures
parent_type: CourseSection
related_resources_text: ''
resource_index_text: ''
resourcetype: Video
start_time: ''
title: 'Lecture 27: Backpropagation: Find Partial Derivatives'
uid: a11969b7-2781-9223-9726-9718da7c7cb1
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture27_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/7656fc94359b5298b8496f66bd7ad833_lZrIPRnoGQQ.vtt
  video_thumbnail_file: https://img.youtube.com/vi/lZrIPRnoGQQ/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/f746508a6b1dd40c702cc4424382c7ba_lZrIPRnoGQQ.pdf
video_metadata:
  youtube_id: lZrIPRnoGQQ
---

Description
-----------

In this lecture, Professor Strang presents Professor Sraâ€™s theorem which proves the convergence of stochastic gradient descent (SGD). He then reviews backpropagation, a method to compute derivatives quickly, using the chain rule.

Summary
-------

Computational graph: Each step in computing \\(F(x)\\) from the weights  
Derivative of each step + chain rule gives gradient of \\(F\\).  
Reverse mode: Backwards from output to input  
The key step to optimizing weights is backprop + stoch grad descent.

Related section in textbook: VII.3

**Instructor:** Prof. Gilbert Strang

