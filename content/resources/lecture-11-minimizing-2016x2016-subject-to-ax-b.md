---
content_type: resource
description: ''
end_time: ''
file: null
learning_resource_types:
- Lecture Videos
license: https://creativecommons.org/licenses/by-nc-sa/4.0/
ocw_type: ''
optional_tab_title: Problem Set
optional_text: "**Problems for Lecture 11  \nFrom textbook Section I.11**\n\n6\\.\
  \ The first page of I.11 shows _unit balls_ for the \\\\(\\\\ell^1\\\\) and \\\\\
  (\\\\ell^2\\\\) and \\\\(\\\\ell^\\\\infty\\\\) norms. Those are the three sets\
  \ of vectors \\\\(\\\\boldsymbol{v}=(v\\_1,v\\_2)\\\\) with \\\\(||\\\\boldsymbol{v}||\\\
  _1\\\\leq 1, ||\\\\boldsymbol{v}||\\_2\\\\leq 1, ||\\\\boldsymbol{v}||\\_\\\\infty\\\
  \\leq 1\\\\). _Unit balls are always convex because of the triangle inequality for\
  \ vector norms_:\n\n$$ \\\\text{If } ||\\\\boldsymbol{v}||\\\\leq 1\\\\text{ and\
  \ } ||\\\\boldsymbol{w}||\\\\leq1 \\\\text{ show that } ||\\\\frac{\\\\boldsymbol{v}}{2}+\\\
  \\frac{\\\\boldsymbol{w}}{2}||\\\\leq 1$$\n\n10\\. What multiple of \\\\(\\\\boldsymbol{a}\
  \ = \\\\left\\[\\\\begin{matrix}1\\\\\\\\1\\\\end{matrix}\\\\right\\]\\\\) should\
  \ be subtracted from \\\\(\\\\boldsymbol{b} = \\\\left\\[\\\\begin{matrix}4\\\\\\\
  \\0\\\\end{matrix}\\\\right\\]\\\\) to make the result \\\\(\\\\boldsymbol{A}\\\
  _2\\\\) orthogonal to \\\\(\\\\boldsymbol{a}\\\\)? Sketch a figure to show \\\\\
  (\\\\boldsymbol{a}\\\\), \\\\(\\\\boldsymbol{b}\\\\), and \\\\(\\\\boldsymbol{A}\\\
  _2\\\\)."
parent_title: Video Lectures
parent_type: CourseSection
related_resources_text: ''
resource_index_text: ''
resourcetype: Video
start_time: ''
title: "Lecture 11: Minimizing \u2016x\u2016 Subject to Ax = b "
uid: 57520c26-1eb1-b9ca-bc44-60a953a67428
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture11_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/166e9930bc7f53a89f94206db8661367_MuEW9pG9oxE.vtt
  video_thumbnail_file: https://img.youtube.com/vi/MuEW9pG9oxE/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/f8ece4b5c1f0a1c6d1ff79a17e0162e2_MuEW9pG9oxE.pdf
video_metadata:
  youtube_id: MuEW9pG9oxE
---

Description
-----------

In this lecture, Professor Strang revisits the ways to solve least squares problems. In particular, he focuses on the Gram-Schmidt process that finds orthogonal vectors.

Summary
-------

Picture the shortest \\(x\\) in \\(\\ell^1\\) and \\(\\ell^2\\) and \\(\\ell^\\infty\\) norms  
The \\(\\ell^1\\) norm gives a sparse solution \\(x\\).  
Details of Gram-Schmidt orthogonalization and \\(A = QR\\)  
Orthogonal vectors in \\(Q\\) from independent vectors in \\(A\\)

Related section in textbook: I.11

**Instructor:** Prof. Gilbert Strang

