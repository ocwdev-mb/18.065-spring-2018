---
content_type: resource
description: ''
end_time: ''
file: null
learning_resource_types:
- Lecture Videos
license: https://creativecommons.org/licenses/by-nc-sa/4.0/
ocw_type: ''
optional_tab_title: Problem Set
optional_text: "**Problems for Lecture 18  \nFrom textbook Section III.2**\n\n4\\\
  . \\\\(S\\\\) is a symmetric matrix with eigenvalues \\\\(\\\\lambda\\_1>\\\\lambda\\\
  _2>\\\\ldots>\\\\lambda\\_n\\\\) and eigenvectors \\\\(\\\\boldsymbol{q}\\_1,\\\\\
  boldsymbol{q}\\_2,\\\\ldots,\\\\boldsymbol{q}\\_n\\\\). Which \\\\(i\\\\) of those\
  \ eigenvectors are a basis for an \\\\(i\\\\)-dimensional subspace \\\\(Y\\\\) with\
  \ this property: The minimum of \\\\(\\\\boldsymbol{x}^{\\\\mathtt{T}} S\\\\boldsymbol{x}/\\\
  \\boldsymbol{x}^{\\\\mathtt{T}} \\\\boldsymbol{x}\\\\) for \\\\(\\\\boldsymbol{x}\\\
  \\) in \\\\(Y\\\\) is \\\\(\\\\lambda\\_i\\\\)?\n\n10\\. Show that this \\\\(2n\\\
  \\times 2n\\\\) KKT matrix \\\\(H\\\\) has \\\\(n\\\\) positive and \\\\(n\\\\)\
  \ negative eigenvalues:\n\n$$  \n\\\\begin{array}{c}\\\\boldsymbol{S}\\\\textbf{\
  \ positive definite}\\\\\\\\\\\\boldsymbol{C}\\\\textbf{ invertible}\\\\end{array}\\\
  \\qquad \\\\boldsymbol{H}=\\\\left\\[\\\\begin{array}{cc}\\\\boldsymbol{S}&\\\\\
  boldsymbol{C}\\\\\\\\\\\\boldsymbol{C}^{\\\\mathbf{T}}&\\\\mathbf{0}\\\\\\\\\\\\\
  end{array}\\\\right\\]  \n$$\n\nThe first \\\\(n\\\\) pivots from \\\\(S\\\\) are\
  \ positive. The last \\\\(n\\\\) pivots come from \\\\(-C^{\\\\mathtt{T}}S^{-1}C\\\
  \\)."
parent_title: Video Lectures
parent_type: CourseSection
related_resources_text: ''
resource_index_text: ''
resourcetype: Video
start_time: ''
title: 'Lecture 18: Counting Parameters in SVD, LU, QR, Saddle Points'
uid: 372ba297-7de9-51f2-5703-5e929fb72095
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture18_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/6820501a14615c2299bedafa0a7093de_xaSL8yFgqig.vtt
  video_thumbnail_file: https://img.youtube.com/vi/xaSL8yFgqig/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/ade37003589b2c2d0030ea38cea605b0_xaSL8yFgqig.pdf
video_metadata:
  youtube_id: xaSL8yFgqig
---

Description
-----------

In this lecture, Professor Strang reviews counting the free parameters in a variety of key matrices. He then moves on to finding saddle points from constraints and Lagrange multipliers.

Summary
-------

**Topic 1:** Find _\\(n^2\\)_Â parameters in \\(L\\) and \\(U\\), \\(Q\\) and \\(R\\), ...  
Find \\((m + n - r)r\\) parameters in a matrix of rank \\(r\\)  
**Topic 2:** Find saddle points from constraints and Lagrange multipliers

Related section in textbook: III.2

**Instructor:** Prof. Gilbert Strang

