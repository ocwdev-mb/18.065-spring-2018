---
content_type: resource
description: ''
end_time: ''
file: null
learning_resource_types:
- Lecture Videos
license: https://creativecommons.org/licenses/by-nc-sa/4.0/
ocw_type: ''
optional_tab_title: Problem Set
optional_text: "**Problems for Lecture 4**  \n**From textbook Section I.6**\n\n2\\\
  . Compute the eigenvalues and eigenvectors of _A_ and _A_{{< sup \"\u22121\" >}}.\
  \ Check the trace!\n\n\_ \_ \__A_ = \\\\(\\\\left\\[\\\\begin{array}{cc}0&2\\\\\\\
  \\1&1\\\\\\\\\\\\end{array}\\\\right\\]\\\\) \_\_\_\_ and \_\_\_\_ _A_{{< sup \"\
  \u22121\" >}} =\_\\\\(\\\\left\\[\\\\begin{array}{cc}-1/2&1\\\\\\\\1/2&0\\\\\\\\\
  \\\\end{array}\\\\right\\]\\\\) .\n\n_A_{{< sup \"\u22121\" >}} has the \\_\\_\\\
  _\\_ eigenvectors as _A_. When _A_ has eigenvalues _\u03BB_{{< sub \"1\" >}} and\
  \ _\u03BB_{{< sub \"2\" >}}, its inverse has eigenvalues \\_\\_\\_\\_.\n\n11\\.\
  \ **_The eigenvalues of A equal the eigenvalues of A_**{{< sup \"**T**\" >}}. This\
  \ is because det(_A_ \u2212 _\u03BBI_) equals det(_A_{{< sup \"T\" >}} \u2212 _\u03BB\
  I_). That is true because \\_\\_\\_\\_. Show by an example that the eigenvectors\
  \ of _A_ and _A_{{< sup \"T\" >}} are _not_ the same.\n\n15\\. (a) Factor these\
  \ two matrices into _A_ = _X_\x03\u039B_X_{{< sup \"\u22121\" >}}:\n\n\_ \_ \__A_\
  \ = \\\\(\\\\left\\[\\\\begin{array}{cc}1&2\\\\\\\\0&3\\\\\\\\\\\\end{array}\\\\\
  right\\]\\\\) \_\_\_\_ and \_\_\_\_ _A_\_=\_\\\\(\\\\left\\[\\\\begin{array}{cc}1&1\\\
  \\\\\\3&3\\\\\\\\\\\\end{array}\\\\right\\]\\\\) . \x14\n\n(b) If _A_ = _X_\x03\u039B\
  _X_{{< sup \"\u22121\" >}} then _A_{{< sup \"3\" >}} = ( )( )( ) and\__A_{{< sup\
  \ \"\u22121\" >}} = ( )( )( )."
parent_title: Video Lectures
parent_type: CourseSection
related_resources_text: ''
resource_index_text: ''
resourcetype: Video
start_time: ''
title: 'Lecture 4: Eigenvalues and Eigenvectors'
uid: 8540e74d-ad3d-8809-1ed4-aa04d359c63d
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture04_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/9ea80b7ada3c56098596eae487e32e52_k095NdrHxY4.vtt
  video_thumbnail_file: https://img.youtube.com/vi/k095NdrHxY4/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/a79157d515aaa07ade32a04e56ad8d1a_k095NdrHxY4.pdf
video_metadata:
  youtube_id: k095NdrHxY4
---

Description
-----------

Professor Strang begins this lecture talking about eigenvectors and eigenvalues and why they are useful. Then he moves to a discussion of symmetric matrices, in particular, positive definite matrices.

Summary
-------

\\(Ax =\\) eigenvalue times \\(x\\)  
\\(A^2xÂ =\\) (eigenvalue)\\(^2\\) times \\(x\\)  
Write other vectors as combinations of eigenvectors  
Similar matrix \\(B = M^{-1}AM\\) has the same eigenvalues as \\(A\\)

Related section in textbook: I.6

**Instructor:** Prof. Gilbert Strang

