---
content_type: resource
description: ''
end_time: ''
file: null
learning_resource_types:
- Lecture Videos
license: https://creativecommons.org/licenses/by-nc-sa/4.0/
ocw_type: ''
optional_tab_title: Problem Set
optional_text: "**Problems for Lecture 10  \nFrom textbook Introduction Chapter 2**\n\
  \nProblems 12 and 17 use four data points \\\\(\\\\boldsymbol{b}\\\\) = (0, 8, 8,\
  \ 20) to bring out the key ideas.\n\n![Fig_II-3.jpg]({{< resource_file 25fc4da3-a8de-7b3b-4def-2954481cf268\
  \ >}})\n\n\\\\(\\\\hspace{10pt}\\\\)Figure II.3: The closest line \\\\(C + Dt\\\\\
  ) in the \\\\(t \u2212 b\\\\) plane matches \\\\(C\\\\boldsymbol{a}\\_1 + D\\\\\
  boldsymbol{a}\\_2\\\\) in\_**R**{{< sup \"4\" >}}.\n\n12\\. With \\\\(b\\\\) = 0,\
  \ 8, 8, 20 at \\\\(t\\\\) = 0, 1, 3, 4, set up and solve the normal equations \\\
  \\(A^{\\\\mathtt{T}}A\\\\widehat {\\\\boldsymbol{x}} = A^{\\\\mathtt{T}}\\\\boldsymbol{b}\\\
  \\). For the best straight line in Figure II.3a, find its four heights \\\\(p\\\
  _i\\\\) and four errors \\\\(e\\_i\\\\). What is the minimum squared error \\\\\
  (E=e^2\\_1+ e^2\\_2+e^2\\_3+e^2\\_4\\\\) ?\n\n17\\. Project \\\\(\\\\boldsymbol{b}\\\
  \\) = (0, 8, 8, 20) onto the line through \\\\(\\\\boldsymbol{a}\\\\) = (1, 1, 1,\
  \ 1). Find \\\\(\\\\widehat\_ x = \\\\boldsymbol{a}^{\\\\mathtt{T}}\\\\boldsymbol{b}/\\\
  \\boldsymbol{a}^{\\\\mathtt{T}}\\\\boldsymbol{a}\\\\) and the projection \\\\(\\\
  \\boldsymbol{p} = \\\\widehat x \\\\boldsymbol{a}\\\\). Check that \\\\(\\\\boldsymbol{e}\
  \ = \\\\boldsymbol{b} \u2212 \\\\boldsymbol{p}\\\\) is perpendicular to \\\\(\\\\\
  boldsymbol{a}\\\\), and find the shortest distance \\\\(\\\\|\\\\boldsymbol{e}\\\
  \\|\\\\) from \\\\(\\\\boldsymbol{b}\\\\) to the line through \\\\(\\\\boldsymbol{a}\\\
  \\)."
parent_title: Video Lectures
parent_type: CourseSection
related_resources_text: ''
resource_index_text: ''
resourcetype: Video
start_time: ''
title: 'Lecture 10: Survey of Difficulties with Ax = b'
uid: 16628e44-82cd-c2a7-5b19-4304b853bbe5
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture10_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/11c2fb70b2995e9d96f9da4c2fb8c773_Z_5uLqcwDgM.vtt
  video_thumbnail_file: https://img.youtube.com/vi/Z_5uLqcwDgM/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/212daa75e3085a82fee93745e3942ff3_Z_5uLqcwDgM.pdf
video_metadata:
  youtube_id: Z_5uLqcwDgM
---

Description
-----------

The subject of this lecture is the matrix equation \\(Ax = b\\). Solving for \\(x\\) presents a number of challenges that must be addressed when doing computations with large matrices.

Summary
-------

Large condition number \\(\\Vert A \\Vert \\ \\Vert A^{-1} \\Vert\\)  
\\(A\\) is ill-conditioned and small errors are amplified.  
Undetermined case \\(m \< n\\) : typical of deep learning  
Penalty method regularizes a singular problem.

Related chapter in textbook: Introduction to Chapter II

**Instructor:** Prof. Gilbert Strang

