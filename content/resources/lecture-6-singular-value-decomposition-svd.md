---
content_type: resource
description: ''
end_time: ''
file: null
learning_resource_types:
- Lecture Videos
license: https://creativecommons.org/licenses/by-nc-sa/4.0/
ocw_type: ''
optional_tab_title: Problem Set
optional_text: "**Problems for Lecture 6  \nFrom textbook Section I.8**\n\n1\\. A\
  \ symmetric matrix \\\\(S=S^{\\\\mathtt{T}}\\\\) has orthonormal eigenvectors \\\
  \\(\\\\boldsymbol{v}\\_1\\\\) to \\\\(\\\\boldsymbol{v}\\_n\\\\). Then any vector\
  \ \\\\(\\\\boldsymbol{x}\\\\) can be written as a combination \\\\(\\\\boldsymbol{x}\
  \ = c\\_1\\\\boldsymbol{v}\\_1 + \xB7 \xB7 \xB7 +c\\_n\\\\boldsymbol{v}\\_n\\\\\
  ). Explain these two formulas:\n\n$$ \\\\boldsymbol{x}^{\\\\mathtt{T}}\\\\boldsymbol{x}\
  \ = {c\\_1}^2+ \xB7 \xB7 \xB7 +{c\\_n}^2 \\\\hspace{12pt} \\\\boldsymbol{x}^{\\\\\
  mathtt{T}} S\\\\boldsymbol{x} = \\\\lambda\\_1{c\\_1}^2+ \xB7 \xB7 \xB7 +\\\\lambda\\\
  _n{c\\_n}^2 $$\n\n6\\. Find the \u03C3\u2019s and \\\\(\\\\boldsymbol{v}\\\\)\u2019\
  s and \\\\(\\\\boldsymbol{u}\\\\)\u2019s in the SVD for \\\\(A =\\\\left\\[\\\\\
  begin{matrix}3 & 4\\\\\\\\ 0 & 5\\\\end{matrix}\\\\right\\]\\\\). Use equation (12)."
parent_title: Video Lectures
parent_type: CourseSection
related_resources_text: ''
resource_index_text: ''
resourcetype: Video
start_time: ''
title: 'Lecture 6: Singular Value Decomposition (SVD)'
uid: 3433b303-aa4b-d769-0c8c-1d76a8edfc50
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture06_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/cbdda6e124ef5ddcb284b3eaa7329669_rYz83XPxiZo.vtt
  video_thumbnail_file: https://img.youtube.com/vi/rYz83XPxiZo/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/e6d220ed61ce3b1ebe002c67e04b63dd_rYz83XPxiZo.pdf
video_metadata:
  youtube_id: rYz83XPxiZo
---

Description
-----------

Singular Value Decomposition (SVD) is the primary topic of this lecture. Professor Strang explains and illustrates how the SVD separates a matrix into rank one pieces, and that those pieces come in order of importance.

Summary
-------

Columns of _V_ are orthonormal eigenvectors of _A_{{< sup "T" >}}_A._  
_A_**_v_** = \\(\\sigma\\)**_u_** gives orthonormal eigenvectors **_u_** of _AA_{{< sup "T" >}}.  
\\(\\sigma^2 =\\) eigenvalue of _A_{{< sup "T" >}}_A_ = eigenvalue of _AA_{{< sup "T" >}} \\( \\neq\\) 0  
_A_ = (rotation)(stretching)(rotation) \\(U\\Sigma\\)_V_{{< sup "T" >}} for every _A_

Related section in textbook: I.8

**Instructor:** Prof. Gilbert Strang

