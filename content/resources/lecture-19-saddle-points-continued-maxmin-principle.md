---
content_type: resource
description: ''
end_time: ''
file: null
learning_resource_types:
- Lecture Videos
license: https://creativecommons.org/licenses/by-nc-sa/4.0/
ocw_type: ''
optional_tab_title: Problem Set
optional_text: "**Problems for Lecture 19  \nFrom textbook Sections III.2 and V.1**\n\
  \n3\\. We know \\\\(\\\\frac{1}{3}\\\\) of all integers are divisible by 3 and \\\
  \\(\\\\frac{1}{7}\\\\) of integers are divisible by 7. What fraction of integers\
  \ will be divisible by 3 or 7 or both?\n\n8\\. Equation (4) gave a second equivalent\
  \ form for \\\\(S^2\\\\) (the variance using samples):\n\n$$ \\\\boldsymbol{S^2}=\\\
  \\frac{1}{N-1} \\\\text{ sum of } (x\\_i-m)^2=\\\\frac{1}{N-1}\\\\left\\[\\\\left(\\\
  \\text{sum of } x^2\\_i\\\\right)-Nm^2\\\\right\\]. $$\n\nVerify the matching identity\
  \ for the expected variance \\\\(\\\\sigma^2\\\\) (using \\\\(m=\\\\Sigma\\\\, p\\\
  _i\\\\, x\\_i\\\\)):\n\n$$ \\\\boldsymbol{\\\\sigma^2}= \\\\textbf{ sum of } \\\\\
  boldsymbol{p\\_i\\\\left(x\\_i-m\\\\right)^2}=\\\\left(\\\\textbf{sum of } \\\\\
  boldsymbol{p\\_i\\\\,x\\_i^2}\\\\right)-\\\\boldsymbol{m^2}. $$"
parent_title: Video Lectures
parent_type: CourseSection
related_resources_text: ''
resource_index_text: ''
resourcetype: Video
start_time: ''
title: 'Lecture 19: Saddle Points Continued, Maxmin Principle'
uid: 1a9b96a6-8484-56dd-2b38-0557ed66a416
video_files:
  archive_url: https://archive.org/download/MIT18.065S18/MIT18_065S18_Lecture19_300k.mp4
  video_captions_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/0115219bc6a75aca92df48dfa5398d22_2K7CvGnebO0.vtt
  video_thumbnail_file: https://img.youtube.com/vi/2K7CvGnebO0/default.jpg
  video_transcript_file: /courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/756ed177370f2e49ffabf9c9e65a16ba_2K7CvGnebO0.pdf
video_metadata:
  youtube_id: 2K7CvGnebO0
---

Description
-----------

Professor Strang continues his discussion of saddle points, which are critical for deep learning applications. Later in the lecture, he reviews the Maxmin Principle, a decision rule used in probability and statistics to optimize outcomes.

Summary
-------

\\(x'Sx/x'x\\) has a saddle at eigenvalues between lowest / highest.  
(Max over all \\(k\\)-dim spaces) of (Min of \\(x'Sx/x'x\\)) = evalue  
Sample mean and expected mean  
Sample variance and \\(k\\){{< sup "th" >}} eigenvalue variance

Related sections in textbook: III.2 and V.1

**Instructor:** Prof. Gilbert Strang

